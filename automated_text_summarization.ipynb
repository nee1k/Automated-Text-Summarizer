{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization - Final",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyoFbahV8DCO"
      },
      "source": [
        "# Preparing the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vk1t0I8XV2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456d463d-d5d7-4c37-dbe6-ee59a2164c5a"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import heapq\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "!pip install feedparser\n",
        "\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import json\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.append('explanation')\n",
        "\n",
        "def clean_html(text):\n",
        "  if text == '':\n",
        "    return ''\n",
        "  else:\n",
        "    return BeautifulSoup(text, 'html5lib').get_text()\n",
        "\n",
        "\n",
        "def visualize(title, sentence_list, best_sentences):\n",
        "  text = ''\n",
        "\n",
        "  display(HTML(f'<h3>{title}</h3>'))\n",
        "  for sentence in sentence_list:\n",
        "    if sentence in best_sentences:\n",
        "      text += ' ' + str(sentence).replace(sentence, f\"<mark>{sentence}</mark>\")\n",
        "    else:\n",
        "      text += ' ' + sentence\n",
        "  display(HTML(f\"\"\" {text} \"\"\"))\n",
        "\n",
        "\n",
        "url = 'https://www.aitrends.com/feed/'\n",
        "feed = feedparser.parse(url)\n",
        "\n",
        "articles = []\n",
        "for e in feed.entries:\n",
        "  articles.append({'title': e.title, 'content': clean_html(e.content[0].value)})\n",
        "\n",
        "save_file = os.path.join('feed.json')\n",
        "feed = open(save_file, 'w+')\n",
        "feed.write(json.dumps(articles, indent=1))\n",
        "feed.close()\n",
        "\n",
        "blog_articles = json.loads(open('feed.json').read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (6.0.8)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser) (1.0.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XMgl673Xho8"
      },
      "source": [
        "def preprocess(text):\n",
        "  formatted_text = text.lower()\n",
        "  tokens = []\n",
        "  for token in nltk.word_tokenize(formatted_text):\n",
        "    tokens.append(token)\n",
        "  tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
        "  formatted_text = ' '.join(element for element in tokens)\n",
        "\n",
        "  return formatted_text\n",
        "\n",
        "\n",
        "def calculate_sentences_score(sentences, important_words, distance):\n",
        "  scores = []\n",
        "  sentence_index = 0\n",
        "\n",
        "  for sentence in [nltk.word_tokenize(sentence) for sentence in sentences]:\n",
        "\n",
        "    word_index = []\n",
        "    for word in important_words:\n",
        "      try:\n",
        "        word_index.append(sentence.index(word))\n",
        "      except ValueError:\n",
        "        pass\n",
        "\n",
        "    word_index.sort()\n",
        "\n",
        "    if len(word_index) == 0:\n",
        "      continue\n",
        "\n",
        "    groups_list = []\n",
        "    group = [word_index[0]]\n",
        "    i = 1 \n",
        "    while i < len(word_index): \n",
        "      if word_index[i] - word_index[i - 1] < distance:\n",
        "        group.append(word_index[i])\n",
        "      else:\n",
        "        groups_list.append(group[:])\n",
        "        group = [word_index[i]]\n",
        "      i += 1\n",
        "    groups_list.append(group)\n",
        "\n",
        "    max_group_score = 0\n",
        "    for g in groups_list:\n",
        "      important_words_in_group = len(g)\n",
        "      total_words_in_group = g[-1] - g[0] + 1\n",
        "      score = 1.0 * important_words_in_group**2 / total_words_in_group\n",
        "\n",
        "      if score > max_group_score:\n",
        "        max_group_score = score\n",
        "\n",
        "    scores.append((max_group_score, sentence_index))\n",
        "    sentence_index += 1\n",
        "\n",
        "  return scores\n",
        "\n",
        "\n",
        "def luhn_summarize(text, top_n_words, distance, number_of_sentences, percentage = 0):\n",
        "  original_sentences = [sentence for sentence in nltk.sent_tokenize(text)]\n",
        "  formatted_sentences = [preprocess(original_sentence) for original_sentence in original_sentences]\n",
        "  words = [word for sentence in formatted_sentences for word in nltk.word_tokenize(sentence)]\n",
        "  frequency = nltk.FreqDist(words)\n",
        "  top_n_words = [word[0] for word in frequency.most_common(top_n_words)]\n",
        "  sentences_score = calculate_sentences_score(formatted_sentences, top_n_words, distance)\n",
        "\n",
        "  if percentage > 0:\n",
        "    best_sentences = heapq.nlargest(int(len(formatted_sentences) * percentage), sentences_score)\n",
        "  else:  \n",
        "    best_sentences = heapq.nlargest(number_of_sentences, sentences_score)\n",
        "\n",
        "  best_sentences = [original_sentences[i] for (score, i) in best_sentences]\n",
        "\n",
        "  return original_sentences, best_sentences, sentences_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pwVqcvj5Rdd"
      },
      "source": [
        "# Summarizing the articles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "for article in blog_articles:\n",
        "    if (len(article['content'])) == 8715:\n",
        "        original_sentences, best_sentences, _ = luhn_summarize(article['content'], 150, 10, number_of_sentences=5, percentage=0.3)\n",
        "        print(\"best_sentences\", len(best_sentences))\n",
        "        visualize(\"Summarization based on Luhn Algorithm\", original_sentences, best_sentences)\n"
      ],
      "metadata": {
        "id": "RmDsT55obKCi",
        "outputId": "8524f7d0-9af7-44cd-8c67-e08189bd7f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_sentences 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Summarization based on Luhn Algorithm</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "  <mark>By John P. Desmond, AI Trends Editor   \n",
              "Two experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the AI World Government event held virtually and in-person this week in Alexandria, Va. \n",
              "Taka Ariga, chief data scientist and director, US Government Accountability Office\n",
              "Taka Ariga, chief data scientist and director at the US Government Accountability Office, described an AI accountability framework he uses within his agency and plans to make available to others.</mark> <mark>And Bryce Goodman, chief strategist for AI and machine learning at the Defense Innovation Unit (DIU), a unit of the Department of Defense founded to help the US military make faster use of emerging commercial technologies, described work in his unit to apply principles of AI development to terminology that an engineer can apply.</mark> <mark>Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, discussed an AI Accountability Framework he helped to develop by convening a forum of experts in the government, industry, nonprofits, as well as federal inspector general officials and AI experts.</mark> “We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said. “GAO is in the business of verification.”  \n",
              "The effort to produce a formal framework began in September 2020 and included 60% women, 40% of whom were underrepresented minorities, to discuss over two days. The effort was spurred by a desire to ground the AI accountability framework in the reality of an engineer’s day-to-day work. <mark>The resulting framework was first published in June as what Ariga described as “version 1.0.”  \n",
              "Seeking to Bring a “High-Altitude Posture” Down to Earth  \n",
              "“We found the AI accountability framework had a very high-altitude posture,” Ariga said.</mark> “These are laudable ideals and aspirations, but what do they mean to the day-to-day AI practitioner? <mark>There is a gap, while we see AI proliferating across the government.”  \n",
              "“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring.</mark> The development effort stands on four “pillars” of Governance, Data, Monitoring and Performance. Governance reviews what the organization has put in place to oversee the AI efforts. “The chief AI officer might be in place, but what does it mean? Can the person make changes? <mark>Is it multidisciplinary?”  At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”  \n",
              "For the Data pillar, his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.</mark> <mark>For the Performance pillar, the team will consider the “societal impact” the AI system will have in deployment, including whether it risks a violation of the Civil Rights Act.</mark> “Auditors have a long-standing track record of evaluating equity. We grounded the evaluation of AI to a proven system,” Ariga said. <mark>Emphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said.</mark> “We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said. He is part of the discussion with NIST on an overall government AI accountability framework. “We don’t want an ecosystem of confusion,” Ariga said. “We want a whole-government approach. <mark>We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”  \n",
              "DIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines  \n",
              "Bryce Goodman, chief strategist for AI and machine learning, the Defense Innovation Unit\n",
              "At the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government.</mark> Projects Goodman has been involved with implementation of AI for humanitarian assistance and disaster response, predictive maintenance, to counter-disinformation, and predictive health. He heads the Responsible AI Working Group. He is a faculty member of Singularity University, has a wide range of consulting clients from inside and outside the government, and holds a PhD in AI and Philosophy from the University of Oxford. <mark>The DOD in February 2020 adopted five areas of Ethical Principles for AI after 15 months of consulting with AI experts in commercial industry, government academia and the American public.</mark> These areas are: Responsible, Equitable, Traceable, Reliable and Governable. <mark>“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event.</mark> <mark>“That’s the gap we are trying to fill.” \n",
              "Before the DIU even considers a project, they run through the ethical principles to see if it passes muster.</mark> Not all projects do. “There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said. <mark>All project stakeholders, including from commercial vendors and within the government, need to be able to test and validate and go beyond minimum legal requirements to meet the principles.</mark> “The law is not moving as fast as AI, which is why these principles are important,” he said. Also, collaboration is going on across the government to ensure values are being preserved and maintained. “Our intention with these guidelines is not to try to achieve perfection, but to avoid catastrophic consequences,” Goodman said. <mark>“It can be difficult to get a group to agree on what the best outcome is, but it’s easier to get the group to agree on what the worst-case outcome is.”  \n",
              "The DIU guidelines along with case studies and supplemental materials will be published on the DIU website “soon,” Goodman said, to help others leverage the experience.</mark> <mark>Here are Questions DIU Asks Before Development Starts  \n",
              "The first step in the guidelines is to define the task.</mark> “That’s the single most important question,” he said. <mark>“Only if there is an advantage, should you use AI.” \n",
              "Next is a benchmark, which needs to be set up front to know if the project has delivered.</mark> Next, he evaluates ownership of the candidate data. <mark>“Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said.</mark> “We need a certain contract on who owns the data. <mark>If ambiguous, this can lead to problems.”  \n",
              "Next, Goodman’s team wants a sample of data to evaluate.</mark> Then, they need to know how and why the information was collected. “If consent was given for one purpose, we cannot use it for another purpose without re-obtaining consent,” he said. Next, the team asks if the responsible stakeholders are identified, such as pilots who could be affected if a component fails. Next, the responsible mission-holders must be identified. <mark>“We need a single individual for this,” Goodman said.</mark> “Often we have a tradeoff between the performance of an algorithm and its explainability. We might have to decide between the two. Those kinds of decisions have an ethical component and an operational component. So we need to have someone who is accountable for those decisions, which is consistent with the chain of command in the DOD.”   \n",
              "Finally, the DIU team requires a process for rolling back if things go wrong. “We need to be cautious about abandoning the previous system,” he said. Once all these questions are answered in a satisfactory way, the team moves on to the development phase. In lessons learned, Goodman said, “Metrics are key. And simply measuring accuracy might not be adequate. We need to be able to measure success.” \n",
              "Also, fit the technology to the task. “High risk applications require low-risk technology. And when potential harm is significant, we need to have high confidence in the technology,” he said. Another lesson learned is to set expectations with commercial vendors. “We need vendors to be transparent,” he said. ”When someone says they have a proprietary algorithm they cannot tell us about, we are very wary. We view the relationship as a collaboration. It’s the only way we can ensure that the AI is developed responsibly.”  \n",
              "Lastly, “AI is not magic. <mark>It will not solve everything.</mark> It should only be used when necessary and only when we can prove it will provide an advantage.”  \n",
              "Learn more at AI World Government, at the Government Accountability Office, at the AI Accountability Framework and at the Defense Innovation Unit site. "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_summarize(text, number_of_sentences, percentage):\n",
        "  original_text = text\n",
        "  formatted_text = preprocess(original_text)\n",
        "\n",
        "  word_frequency = nltk.FreqDist(nltk.word_tokenize(formatted_text))\n",
        "  highest_frequency = max(word_frequency.values())\n",
        "  for word in word_frequency.keys():\n",
        "    word_frequency[word] = (word_frequency[word] / highest_frequency)\n",
        "  sentence_list = nltk.sent_tokenize(original_text)\n",
        "  \n",
        "  score_sentences = {}\n",
        "  for sentence in sentence_list:\n",
        "    for word in nltk.word_tokenize(sentence):\n",
        "      if word in word_frequency.keys():\n",
        "        if sentence not in score_sentences.keys():\n",
        "          score_sentences[sentence] = word_frequency[word]\n",
        "        else:\n",
        "          score_sentences[sentence] += word_frequency[word]\n",
        "\n",
        "  import heapq\n",
        "  if percentage > 0:\n",
        "    best_sentences = heapq.nlargest(int(len(sentence_list) * percentage), score_sentences, key=score_sentences.get)\n",
        "  else:\n",
        "    best_sentences = heapq.nlargest(number_of_sentences, score_sentences, key=score_sentences.get)\n",
        "\n",
        "  return sentence_list, best_sentences, word_frequency, score_sentences\n",
        "\n",
        "\n",
        "for article in blog_articles:\n",
        "    if (len(article['content'])) == 8715:\n",
        "        original_sentences, best_sentences, _, _ = freq_summarize(article['content'], 100, percentage=0.3)\n",
        "        print(\"best_sentences\", len(best_sentences))\n",
        "        visualize(\"Summarization based on Frequency\", original_sentences, best_sentences)"
      ],
      "metadata": {
        "id": "Zrc5P4lrbXiz",
        "outputId": "7796f8f2-f2b1-4375-d918-952f64dd2856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_sentences 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Summarization based on Frequency</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "  <mark>By John P. Desmond, AI Trends Editor   \n",
              "Two experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the AI World Government event held virtually and in-person this week in Alexandria, Va. \n",
              "Taka Ariga, chief data scientist and director, US Government Accountability Office\n",
              "Taka Ariga, chief data scientist and director at the US Government Accountability Office, described an AI accountability framework he uses within his agency and plans to make available to others.</mark> And Bryce Goodman, chief strategist for AI and machine learning at the Defense Innovation Unit (DIU), a unit of the Department of Defense founded to help the US military make faster use of emerging commercial technologies, described work in his unit to apply principles of AI development to terminology that an engineer can apply. Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, discussed an AI Accountability Framework he helped to develop by convening a forum of experts in the government, industry, nonprofits, as well as federal inspector general officials and AI experts. <mark>“We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said.</mark> “GAO is in the business of verification.”  \n",
              "The effort to produce a formal framework began in September 2020 and included 60% women, 40% of whom were underrepresented minorities, to discuss over two days. The effort was spurred by a desire to ground the AI accountability framework in the reality of an engineer’s day-to-day work. <mark>The resulting framework was first published in June as what Ariga described as “version 1.0.”  \n",
              "Seeking to Bring a “High-Altitude Posture” Down to Earth  \n",
              "“We found the AI accountability framework had a very high-altitude posture,” Ariga said.</mark> “These are laudable ideals and aspirations, but what do they mean to the day-to-day AI practitioner? <mark>There is a gap, while we see AI proliferating across the government.”  \n",
              "“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring.</mark> The development effort stands on four “pillars” of Governance, Data, Monitoring and Performance. Governance reviews what the organization has put in place to oversee the AI efforts. “The chief AI officer might be in place, but what does it mean? Can the person make changes? <mark>Is it multidisciplinary?”  At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”  \n",
              "For the Data pillar, his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.</mark> For the Performance pillar, the team will consider the “societal impact” the AI system will have in deployment, including whether it risks a violation of the Civil Rights Act. “Auditors have a long-standing track record of evaluating equity. We grounded the evaluation of AI to a proven system,” Ariga said. <mark>Emphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said.</mark> <mark>“We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said.</mark> He is part of the discussion with NIST on an overall government AI accountability framework. <mark>“We don’t want an ecosystem of confusion,” Ariga said.</mark> “We want a whole-government approach. <mark>We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”  \n",
              "DIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines  \n",
              "Bryce Goodman, chief strategist for AI and machine learning, the Defense Innovation Unit\n",
              "At the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government.</mark> Projects Goodman has been involved with implementation of AI for humanitarian assistance and disaster response, predictive maintenance, to counter-disinformation, and predictive health. He heads the Responsible AI Working Group. He is a faculty member of Singularity University, has a wide range of consulting clients from inside and outside the government, and holds a PhD in AI and Philosophy from the University of Oxford. The DOD in February 2020 adopted five areas of Ethical Principles for AI after 15 months of consulting with AI experts in commercial industry, government academia and the American public. These areas are: Responsible, Equitable, Traceable, Reliable and Governable. <mark>“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event.</mark> <mark>“That’s the gap we are trying to fill.” \n",
              "Before the DIU even considers a project, they run through the ethical principles to see if it passes muster.</mark> Not all projects do. <mark>“There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said.</mark> All project stakeholders, including from commercial vendors and within the government, need to be able to test and validate and go beyond minimum legal requirements to meet the principles. “The law is not moving as fast as AI, which is why these principles are important,” he said. Also, collaboration is going on across the government to ensure values are being preserved and maintained. <mark>“Our intention with these guidelines is not to try to achieve perfection, but to avoid catastrophic consequences,” Goodman said.</mark> <mark>“It can be difficult to get a group to agree on what the best outcome is, but it’s easier to get the group to agree on what the worst-case outcome is.”  \n",
              "The DIU guidelines along with case studies and supplemental materials will be published on the DIU website “soon,” Goodman said, to help others leverage the experience.</mark> Here are Questions DIU Asks Before Development Starts  \n",
              "The first step in the guidelines is to define the task. <mark>“That’s the single most important question,” he said.</mark> “Only if there is an advantage, should you use AI.” \n",
              "Next is a benchmark, which needs to be set up front to know if the project has delivered. Next, he evaluates ownership of the candidate data. <mark>“Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said.</mark> “We need a certain contract on who owns the data. If ambiguous, this can lead to problems.”  \n",
              "Next, Goodman’s team wants a sample of data to evaluate. Then, they need to know how and why the information was collected. <mark>“If consent was given for one purpose, we cannot use it for another purpose without re-obtaining consent,” he said.</mark> Next, the team asks if the responsible stakeholders are identified, such as pilots who could be affected if a component fails. Next, the responsible mission-holders must be identified. <mark>“We need a single individual for this,” Goodman said.</mark> “Often we have a tradeoff between the performance of an algorithm and its explainability. We might have to decide between the two. Those kinds of decisions have an ethical component and an operational component. So we need to have someone who is accountable for those decisions, which is consistent with the chain of command in the DOD.”   \n",
              "Finally, the DIU team requires a process for rolling back if things go wrong. <mark>“We need to be cautious about abandoning the previous system,” he said.</mark> Once all these questions are answered in a satisfactory way, the team moves on to the development phase. In lessons learned, Goodman said, “Metrics are key. And simply measuring accuracy might not be adequate. We need to be able to measure success.” \n",
              "Also, fit the technology to the task. “High risk applications require low-risk technology. And when potential harm is significant, we need to have high confidence in the technology,” he said. Another lesson learned is to set expectations with commercial vendors. <mark>“We need vendors to be transparent,” he said.</mark> ”When someone says they have a proprietary algorithm they cannot tell us about, we are very wary. We view the relationship as a collaboration. It’s the only way we can ensure that the AI is developed responsibly.”  \n",
              "Lastly, “AI is not magic. It will not solve everything. It should only be used when necessary and only when we can prove it will provide an advantage.”  \n",
              "Learn more at AI World Government, at the Government Accountability Office, at the AI Accountability Framework and at the Defense Innovation Unit site. "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import networkx as nx\n",
        "from nltk.cluster.util import cosine_distance\n",
        "\n",
        "def calculate_sentence_similarity(sentence1, sentence2):\n",
        "  words1 = [word for word in nltk.word_tokenize(sentence1)]\n",
        "  words2 = [word for word in nltk.word_tokenize(sentence2)]\n",
        "\n",
        "  all_words = list(set(words1 + words2))\n",
        "\n",
        "  vector1 = [0] * len(all_words)\n",
        "  vector2 = [0] * len(all_words)\n",
        "\n",
        "  for word in words1: # Bag of words\n",
        "    vector1[all_words.index(word)] += 1\n",
        "\n",
        "  for word in words2:\n",
        "    vector2[all_words.index(word)] += 1\n",
        "\n",
        "  return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "\n",
        "def calculate_similarity_matrix(sentences):\n",
        "  similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "  \n",
        "  for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      similarity_matrix[i][j] = calculate_sentence_similarity(sentences[i], sentences[j])\n",
        "  return similarity_matrix\n",
        "  \n",
        "\n",
        "def cosine_summarize(text, number_of_sentences, percentage):\n",
        "  original_sentences = [sentence for sentence in nltk.sent_tokenize(text)]\n",
        "  formatted_sentences = [preprocess(original_sentence) for original_sentence in original_sentences]\n",
        "  similarity_matrix = calculate_similarity_matrix(formatted_sentences)\n",
        "\n",
        "  similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
        "\n",
        "  scores = nx.pagerank(similarity_graph)\n",
        "  ordered_scores = sorted(((scores[i], score) for i, score in enumerate(original_sentences)), reverse=True)\n",
        "\n",
        "  if percentage > 0:\n",
        "    number_of_sentences = int(len(formatted_sentences) * percentage)\n",
        "\n",
        "  best_sentences = []\n",
        "  for sentence in range(number_of_sentences):\n",
        "    best_sentences.append(ordered_scores[sentence][1])\n",
        "  \n",
        "  return original_sentences, best_sentences, ordered_scores\n",
        "\n",
        "for article in blog_articles:\n",
        "    if (len(article['content'])) == 8715:\n",
        "        original_sentences, best_sentences, ordered_scores = cosine_summarize(article['content'], 100, percentage=0.3)\n",
        "        print(\"original_sentences\", len(original_sentences))\n",
        "        print(\"best_sentences\", len(best_sentences))\n",
        "        visualize(\"Summarization based on Cosine Similarity\", original_sentences, best_sentences)"
      ],
      "metadata": {
        "id": "VKHRxhFCeNeJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "outputId": "3c59a4b7-c37e-4a18-87a7-622c8fc1e093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_sentences 67\n",
            "best_sentences 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Summarization based on Cosine Similarity</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "  By John P. Desmond, AI Trends Editor   \n",
              "Two experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the AI World Government event held virtually and in-person this week in Alexandria, Va. \n",
              "Taka Ariga, chief data scientist and director, US Government Accountability Office\n",
              "Taka Ariga, chief data scientist and director at the US Government Accountability Office, described an AI accountability framework he uses within his agency and plans to make available to others. And Bryce Goodman, chief strategist for AI and machine learning at the Defense Innovation Unit (DIU), a unit of the Department of Defense founded to help the US military make faster use of emerging commercial technologies, described work in his unit to apply principles of AI development to terminology that an engineer can apply. Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, discussed an AI Accountability Framework he helped to develop by convening a forum of experts in the government, industry, nonprofits, as well as federal inspector general officials and AI experts. <mark>“We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said.</mark> “GAO is in the business of verification.”  \n",
              "The effort to produce a formal framework began in September 2020 and included 60% women, 40% of whom were underrepresented minorities, to discuss over two days. The effort was spurred by a desire to ground the AI accountability framework in the reality of an engineer’s day-to-day work. <mark>The resulting framework was first published in June as what Ariga described as “version 1.0.”  \n",
              "Seeking to Bring a “High-Altitude Posture” Down to Earth  \n",
              "“We found the AI accountability framework had a very high-altitude posture,” Ariga said.</mark> “These are laudable ideals and aspirations, but what do they mean to the day-to-day AI practitioner? <mark>There is a gap, while we see AI proliferating across the government.”  \n",
              "“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring.</mark> The development effort stands on four “pillars” of Governance, Data, Monitoring and Performance. Governance reviews what the organization has put in place to oversee the AI efforts. <mark>“The chief AI officer might be in place, but what does it mean?</mark> Can the person make changes? <mark>Is it multidisciplinary?”  At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”  \n",
              "For the Data pillar, his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.</mark> For the Performance pillar, the team will consider the “societal impact” the AI system will have in deployment, including whether it risks a violation of the Civil Rights Act. “Auditors have a long-standing track record of evaluating equity. <mark>We grounded the evaluation of AI to a proven system,” Ariga said.</mark> <mark>Emphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said.</mark> <mark>“We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said.</mark> He is part of the discussion with NIST on an overall government AI accountability framework. <mark>“We don’t want an ecosystem of confusion,” Ariga said.</mark> “We want a whole-government approach. <mark>We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”  \n",
              "DIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines  \n",
              "Bryce Goodman, chief strategist for AI and machine learning, the Defense Innovation Unit\n",
              "At the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government.</mark> Projects Goodman has been involved with implementation of AI for humanitarian assistance and disaster response, predictive maintenance, to counter-disinformation, and predictive health. He heads the Responsible AI Working Group. He is a faculty member of Singularity University, has a wide range of consulting clients from inside and outside the government, and holds a PhD in AI and Philosophy from the University of Oxford. The DOD in February 2020 adopted five areas of Ethical Principles for AI after 15 months of consulting with AI experts in commercial industry, government academia and the American public. These areas are: Responsible, Equitable, Traceable, Reliable and Governable. <mark>“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event.</mark> “That’s the gap we are trying to fill.” \n",
              "Before the DIU even considers a project, they run through the ethical principles to see if it passes muster. Not all projects do. <mark>“There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said.</mark> All project stakeholders, including from commercial vendors and within the government, need to be able to test and validate and go beyond minimum legal requirements to meet the principles. <mark>“The law is not moving as fast as AI, which is why these principles are important,” he said.</mark> Also, collaboration is going on across the government to ensure values are being preserved and maintained. “Our intention with these guidelines is not to try to achieve perfection, but to avoid catastrophic consequences,” Goodman said. “It can be difficult to get a group to agree on what the best outcome is, but it’s easier to get the group to agree on what the worst-case outcome is.”  \n",
              "The DIU guidelines along with case studies and supplemental materials will be published on the DIU website “soon,” Goodman said, to help others leverage the experience. Here are Questions DIU Asks Before Development Starts  \n",
              "The first step in the guidelines is to define the task. <mark>“That’s the single most important question,” he said.</mark> <mark>“Only if there is an advantage, should you use AI.” \n",
              "Next is a benchmark, which needs to be set up front to know if the project has delivered.</mark> Next, he evaluates ownership of the candidate data. <mark>“Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said.</mark> “We need a certain contract on who owns the data. If ambiguous, this can lead to problems.”  \n",
              "Next, Goodman’s team wants a sample of data to evaluate. Then, they need to know how and why the information was collected. “If consent was given for one purpose, we cannot use it for another purpose without re-obtaining consent,” he said. Next, the team asks if the responsible stakeholders are identified, such as pilots who could be affected if a component fails. Next, the responsible mission-holders must be identified. <mark>“We need a single individual for this,” Goodman said.</mark> “Often we have a tradeoff between the performance of an algorithm and its explainability. We might have to decide between the two. Those kinds of decisions have an ethical component and an operational component. So we need to have someone who is accountable for those decisions, which is consistent with the chain of command in the DOD.”   \n",
              "Finally, the DIU team requires a process for rolling back if things go wrong. <mark>“We need to be cautious about abandoning the previous system,” he said.</mark> Once all these questions are answered in a satisfactory way, the team moves on to the development phase. In lessons learned, Goodman said, “Metrics are key. And simply measuring accuracy might not be adequate. We need to be able to measure success.” \n",
              "Also, fit the technology to the task. “High risk applications require low-risk technology. And when potential harm is significant, we need to have high confidence in the technology,” he said. Another lesson learned is to set expectations with commercial vendors. <mark>“We need vendors to be transparent,” he said.</mark> ”When someone says they have a proprietary algorithm they cannot tell us about, we are very wary. We view the relationship as a collaboration. <mark>It’s the only way we can ensure that the AI is developed responsibly.”  \n",
              "Lastly, “AI is not magic.</mark> It will not solve everything. It should only be used when necessary and only when we can prove it will provide an advantage.”  \n",
              "Learn more at AI World Government, at the Government Accountability Office, at the AI Accountability Framework and at the Defense Innovation Unit site. "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgdqVwdN11oe",
        "outputId": "beceb506-48e6-44fe-d915-9356b270976f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['“We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said.',\n",
              " '“Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said.',\n",
              " '“We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said.',\n",
              " 'The resulting framework was first published in June as what Ariga described as “version 1.0.”\\xa0\\xa0\\nSeeking to Bring a “High-Altitude Posture” Down to Earth\\xa0\\xa0\\n“We found the AI accountability framework had a very high-altitude posture,” Ariga said.',\n",
              " 'It’s the only way we can ensure\\xa0that the AI\\xa0is developed responsibly.”\\xa0\\xa0\\nLastly, “AI is not magic.',\n",
              " '“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event.',\n",
              " '“The law is not moving as fast as AI, which is why these principles are important,” he said.',\n",
              " '“We need vendors to be transparent,” he said.',\n",
              " 'Emphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said.',\n",
              " '“We need a single individual for this,” Goodman said.',\n",
              " '“There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said.',\n",
              " 'There is a gap, while we see AI proliferating across the government.”\\xa0\\xa0\\n“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring.',\n",
              " '“That’s the single most important question,” he said.',\n",
              " '“We need to be cautious about abandoning the previous system,” he said.',\n",
              " '“Only if there is an advantage, should you use AI.”\\xa0\\nNext is a benchmark, which needs to be set up front to know if the project\\xa0has delivered.',\n",
              " '“We don’t want an ecosystem of confusion,” Ariga said.',\n",
              " 'We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”\\xa0\\xa0\\nDIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines\\xa0\\xa0\\nBryce Goodman, chief strategist for AI and machine learning, the\\xa0Defense Innovation Unit\\nAt the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government.',\n",
              " '“The chief AI officer might be in place, but what does it mean?',\n",
              " 'We grounded the evaluation of AI to a proven system,” Ariga said.',\n",
              " 'Is it multidisciplinary?”\\xa0 At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”\\xa0\\xa0\\nFor the Data pillar,\\xa0his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_para(sentence_list, best_sentences):\n",
        "    # summarized_text = ''\n",
        "    original_text = ''\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "        print(sentence)\n",
        "\n",
        "        # if sentence in best_sentences:\n",
        "        #     summarized_text += str(sentence) # .replace(sentence, f\"<mark>{sentence}</mark>\")\n",
        "        # else:\n",
        "        #     continue\n",
        "            # text += ' ' + sentence\n",
        "\n",
        "    # print(summarized_text)\n",
        "    # print(\"---\")\n",
        "    # print(original_text)\n",
        "\n",
        "\n",
        "convert_to_para(original_sentences, best_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXTnTKBGf9Yb",
        "outputId": "5934b774-ce60-4706-85ef-ee9c890e8007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By John P. Desmond, AI Trends Editor   \n",
            "Two experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the AI World Government event held virtually and in-person this week in Alexandria, Va. \n",
            "Taka Ariga, chief data scientist and director, US Government Accountability Office\n",
            "Taka Ariga, chief data scientist and director at the US Government Accountability Office, described an AI accountability framework he uses within his agency and plans to make available to others.\n",
            "And Bryce Goodman, chief strategist for AI and machine learning at the Defense Innovation Unit (DIU), a unit of the Department of Defense founded to help the US military make faster use of emerging commercial technologies, described work in his unit to apply principles of AI development to terminology that an engineer can apply.\n",
            "Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, discussed an AI Accountability Framework he helped to develop by convening a forum of experts in the government, industry, nonprofits, as well as federal inspector general officials and AI experts.\n",
            "“We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said.\n",
            "“GAO is in the business of verification.”  \n",
            "The effort to produce a formal framework began in September 2020 and included 60% women, 40% of whom were underrepresented minorities, to discuss over two days.\n",
            "The effort was spurred by a desire to ground the AI accountability framework in the reality of an engineer’s day-to-day work.\n",
            "The resulting framework was first published in June as what Ariga described as “version 1.0.”  \n",
            "Seeking to Bring a “High-Altitude Posture” Down to Earth  \n",
            "“We found the AI accountability framework had a very high-altitude posture,” Ariga said.\n",
            "“These are laudable ideals and aspirations, but what do they mean to the day-to-day AI practitioner?\n",
            "There is a gap, while we see AI proliferating across the government.”  \n",
            "“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring.\n",
            "The development effort stands on four “pillars” of Governance, Data, Monitoring and Performance.\n",
            "Governance reviews what the organization has put in place to oversee the AI efforts.\n",
            "“The chief AI officer might be in place, but what does it mean?\n",
            "Can the person make changes?\n",
            "Is it multidisciplinary?”  At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”  \n",
            "For the Data pillar, his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.\n",
            "For the Performance pillar, the team will consider the “societal impact” the AI system will have in deployment, including whether it risks a violation of the Civil Rights Act.\n",
            "“Auditors have a long-standing track record of evaluating equity.\n",
            "We grounded the evaluation of AI to a proven system,” Ariga said.\n",
            "Emphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said.\n",
            "“We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said.\n",
            "He is part of the discussion with NIST on an overall government AI accountability framework.\n",
            "“We don’t want an ecosystem of confusion,” Ariga said.\n",
            "“We want a whole-government approach.\n",
            "We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”  \n",
            "DIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines  \n",
            "Bryce Goodman, chief strategist for AI and machine learning, the Defense Innovation Unit\n",
            "At the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government.\n",
            "Projects Goodman has been involved with implementation of AI for humanitarian assistance and disaster response, predictive maintenance, to counter-disinformation, and predictive health.\n",
            "He heads the Responsible AI Working Group.\n",
            "He is a faculty member of Singularity University, has a wide range of consulting clients from inside and outside the government, and holds a PhD in AI and Philosophy from the University of Oxford.\n",
            "The DOD in February 2020 adopted five areas of Ethical Principles for AI after 15 months of consulting with AI experts in commercial industry, government academia and the American public.\n",
            "These areas are: Responsible, Equitable, Traceable, Reliable and Governable.\n",
            "“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event.\n",
            "“That’s the gap we are trying to fill.” \n",
            "Before the DIU even considers a project, they run through the ethical principles to see if it passes muster.\n",
            "Not all projects do.\n",
            "“There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said.\n",
            "All project stakeholders, including from commercial vendors and within the government, need to be able to test and validate and go beyond minimum legal requirements to meet the principles.\n",
            "“The law is not moving as fast as AI, which is why these principles are important,” he said.\n",
            "Also, collaboration is going on across the government to ensure values are being preserved and maintained.\n",
            "“Our intention with these guidelines is not to try to achieve perfection, but to avoid catastrophic consequences,” Goodman said.\n",
            "“It can be difficult to get a group to agree on what the best outcome is, but it’s easier to get the group to agree on what the worst-case outcome is.”  \n",
            "The DIU guidelines along with case studies and supplemental materials will be published on the DIU website “soon,” Goodman said, to help others leverage the experience.\n",
            "Here are Questions DIU Asks Before Development Starts  \n",
            "The first step in the guidelines is to define the task.\n",
            "“That’s the single most important question,” he said.\n",
            "“Only if there is an advantage, should you use AI.” \n",
            "Next is a benchmark, which needs to be set up front to know if the project has delivered.\n",
            "Next, he evaluates ownership of the candidate data.\n",
            "“Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said.\n",
            "“We need a certain contract on who owns the data.\n",
            "If ambiguous, this can lead to problems.”  \n",
            "Next, Goodman’s team wants a sample of data to evaluate.\n",
            "Then, they need to know how and why the information was collected.\n",
            "“If consent was given for one purpose, we cannot use it for another purpose without re-obtaining consent,” he said.\n",
            "Next, the team asks if the responsible stakeholders are identified, such as pilots who could be affected if a component fails.\n",
            "Next, the responsible mission-holders must be identified.\n",
            "“We need a single individual for this,” Goodman said.\n",
            "“Often we have a tradeoff between the performance of an algorithm and its explainability.\n",
            "We might have to decide between the two.\n",
            "Those kinds of decisions have an ethical component and an operational component.\n",
            "So we need to have someone who is accountable for those decisions, which is consistent with the chain of command in the DOD.”   \n",
            "Finally, the DIU team requires a process for rolling back if things go wrong.\n",
            "“We need to be cautious about abandoning the previous system,” he said.\n",
            "Once all these questions are answered in a satisfactory way, the team moves on to the development phase.\n",
            "In lessons learned, Goodman said, “Metrics are key.\n",
            "And simply measuring accuracy might not be adequate.\n",
            "We need to be able to measure success.” \n",
            "Also, fit the technology to the task.\n",
            "“High risk applications require low-risk technology.\n",
            "And when potential harm is significant, we need to have high confidence in the technology,” he said.\n",
            "Another lesson learned is to set expectations with commercial vendors.\n",
            "“We need vendors to be transparent,” he said.\n",
            "”When someone says they have a proprietary algorithm they cannot tell us about, we are very wary.\n",
            "We view the relationship as a collaboration.\n",
            "It’s the only way we can ensure that the AI is developed responsibly.”  \n",
            "Lastly, “AI is not magic.\n",
            "It will not solve everything.\n",
            "It should only be used when necessary and only when we can prove it will provide an advantage.”  \n",
            "Learn more at AI World Government, at the Government Accountability Office, at the AI Accountability Framework and at the Defense Innovation Unit site.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "SeA1fCU-2Sx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df15fbbd-062c-4326-e1e9-c817a066e706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'f': 0.7567567517604091,\n",
              "   'p': 0.7777777777777778,\n",
              "   'r': 0.7368421052631579},\n",
              "  'rouge-2': {'f': 0.514285709289796, 'p': 0.5294117647058824, 'r': 0.5},\n",
              "  'rouge-l': {'f': 0.7567567517604091,\n",
              "   'p': 0.7777777777777778,\n",
              "   'r': 0.7368421052631579}}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "model_out = \"he began by starting a five person war cabinet and included chamberlain as lord president of the council\"\n",
        "reference = \"he began his premiership by forming a five-man war cabinet which included chamberlain as lord president of the council\"\n",
        "\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(model_out, reference)"
      ],
      "metadata": {
        "id": "VaYv6xtlfFlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumeval\n",
        "!python -m spacy download en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZT2cn1pKjEe",
        "outputId": "f69b6274-eb6d-4722-beb5-d72665b3f945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumeval\n",
            "  Downloading sumeval-0.2.2.tar.gz (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from sumeval) (1.1.3)\n",
            "Collecting sacrebleu>=1.3.2\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.2->sumeval) (1.21.5)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.2->sumeval) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.3.2->sumeval) (2019.12.20)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: sumeval\n",
            "  Building wheel for sumeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sumeval: filename=sumeval-0.2.2-py3-none-any.whl size=54549 sha256=fb517336ec8000b936809925e927304bde8d701571eff22a411fe5c0b06baf3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3f/31/c521bdfba2be7518bd94ba3e8b982812822167cc0497fad192\n",
            "Successfully built sumeval\n",
            "Installing collected packages: portalocker, colorama, sacrebleu, sumeval\n",
            "Successfully installed colorama-0.4.4 portalocker-2.4.0 sacrebleu-2.0.0 sumeval-0.2.2\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 503 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/chakki-works/sumeval\n",
        "#https://github.com/Tian312/awesome-text-summarization\n",
        "\n",
        "from sumeval.metrics.rouge import RougeCalculator\n",
        "\n",
        "refrence_summary = \"So easy, so good and all natural\"\n",
        "model_summary = \"great hot snack\"\n",
        "\n",
        "rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
        "\n",
        "rouge_1 = rouge.rouge_n(\n",
        "            summary=model_summary,\n",
        "            references=refrence_summary,\n",
        "            n=1)\n",
        "\n",
        "rouge_2 = rouge.rouge_n(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary],\n",
        "            n=2)\n",
        "\n",
        "rouge_l = rouge.rouge_l(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary])\n",
        "\n",
        "# You need spaCy to calculate ROUGE-BE\n",
        "\n",
        "rouge_be = rouge.rouge_be(\n",
        "            summary=model_summary,\n",
        "            references=[refrence_summary])\n",
        "\n",
        "print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
        "    rouge_1, rouge_2, rouge_l, rouge_be\n",
        ").replace(\", \", \"\\n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvO9a7_nKjke",
        "outputId": "72bbe930-d193-4756-9143-fbc956c3a271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b.great=(amod)=>snack\n",
            "b.hot=(amod)=>snack\n",
            "ROUGE-1: 0\n",
            "ROUGE-2: 0\n",
            "ROUGE-L: 0\n",
            "ROUGE-BE: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JwfZID_iK4Ex"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}